# -*- coding: utf-8 -*-
"""Prediction_Model_Inference_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VB8fxb8fsPZS96BHQTX4lebsnw4doQ-7
"""

#======================= Requirements =============================
!pip install pynvml GPUtil py-cpuinfo psutil

from google.colab import drive

drive.mount('/content/drive')

#============ imported Lib ========================
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score as cvs, KFold
from sklearn import preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.preprocessing import LabelEncoder, FunctionTransformer, MinMaxScaler, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import SVR
import pickle
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
import re
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
import shutil
import psutil
import math
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('/content/drive/MyDrive/HPE Datasets/Model Input/FinalCombinedInput_v12.csv')
data.dropna(inplace=True)
data = data.reset_index(drop=True)

y = data[['Throughput (Tokens/sec)', 'Latency (ms)']]
x = data.drop(['Throughput (Tokens/sec)', 'Latency (ms)'], axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

'''import numpy as np
import pickle
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer

# ====================== Feature Lists =====================================
categorical_features = [
    'Model', 'CPU', 'GPU', 'Framework', 'Architecture type',
    'Model Type', 'Precision', 'Activation Function'
]

skewed_numerical_features = [
    'FLOPs',
    '# of GPU', 'GPU Memory Total - VRAM (MB)', 'GPU SM Cores', 'GPU CUDA Cores',
    'CPU Total cores (Including Logical cores)', 'CPU Base clock(GHz)',
    'Total Parameters (Millions)', 'Model Size (MB)', #'Number of hidden Layers',
    'Vocabulary Size',
    #'Number of Attention Layers'
]

other_numerical_features = [
    'GPU Graphics clock', 'GPU Memory clock', 'CPU Max Frequency(GHz)',
    'CPU Threads per Core', 'L1 Cache'
]


skewed_transformer = Pipeline(steps=[
    ('log_transform', FunctionTransformer(np.log1p)),
    ('scaler', StandardScaler())
])


numeric_transformer = StandardScaler()


preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ('skew_num', skewed_transformer, skewed_numerical_features),
        ('other_num', numeric_transformer, other_numerical_features)
    ],
    remainder= 'passthrough'
)

# =================== PipeLine ===============================
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])

pipeline.fit(x_train)
# =================== Saving the Pipeline ===============================
with open('pipeline_inference_preprocessor_simulation.pkl', 'wb') as f:
    pickle.dump(pipeline, f)
'''

with open('pipeline_inference_preprocessor_simulation.pkl', 'rb') as f:
    pipeline = pickle.load(f)


x_train_transformed = pipeline.transform(x_train)
x_test_transformed = pipeline.transform(x_test)

'''multi_output_forest = RandomForestRegressor(n_estimators=100, random_state=42)
multi_output_forest.fit(x_train_transformed.astype(np.float64), y_train)

with open('Inference_simulation_model.pkl', 'wb') as f:
    pickle.dump(multi_output_forest, f)'''

with open('Inference_simulation_model.pkl', 'rb') as f:
    multi_output_forest = pickle.load(f)

y_pred = multi_output_forest.predict(x_test_transformed)
print(r2_score(y_test, y_pred))

'''import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error, r2_score

param_grid = {
    'n_estimators': [100, 200, 300],          # Number of trees in the forest
    'max_depth': [10, 20, 30],                # Maximum depth of the tree (None can still be an option)
    'min_samples_split': [2, 5, 10],          # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2, 4],            # Minimum number of samples required at a leaf node
    'max_features': ['sqrt', 'log2', 1.0],    # Number of features to consider. 1.0 means all features.
    'bootstrap': [True]                       # Method of selecting samples for training each tree
}


# --- 3. GridSearchCV Initialization and Execution ---
# Instantiate the Random Forest regressor
multi_output_forest_cv = RandomForestRegressor(random_state=42)

# Instantiate the GridSearchCV object
# cv=5 means 5-fold cross-validation
# n_jobs=-1 uses all available CPU cores
# scoring is changed to 'neg_mean_squared_error'
grid_search = GridSearchCV(
    estimator=multi_output_forest_cv,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2,
    scoring='neg_mean_squared_error'
)

# Fit the grid search to the data
print("Starting GridSearchCV for Regressor... This may take a while.")
grid_search.fit(x_train_transformed, y_train)
print("GridSearchCV finished.")
print("-" * 30)


# --- 4. Results ---
# Print the best parameters found by GridSearchCV
print(f"Best Parameters found: {grid_search.best_params_}")

# The best_score_ is the negative MSE. We can convert it back to positive MSE.
best_mse = -grid_search.best_score_
print(f"Best cross-validation MSE: {best_mse:.4f}")
print(f"Best cross-validation RMSE: {np.sqrt(best_mse):.4f}")
print("-" * 30)


# --- 5. Evaluate the Best Model on the Test Set ---
# The GridSearchCV automatically refits the best model on the whole training set
best_rf_model = grid_search.best_estimator_

best_simulation_model_inference = grid_search.best_estimator_
with open('Inference_simulation_model.pkl', 'wb') as f:
    pickle.dump(best_simulation_model_inference, f)'''

"""# **Model Working**"""

from hw import get_stats

cpu = get_stats()[0]
GPU = get_stats()[1]

cpu['CPU Model'].replace('Intel(R) Xeon(R) CPU @ 2.00GHz', 'Intel(R) Xeon', inplace=True)
cpu['L1 Cache'] = 82

GPU['GPU SM Cores'] = 32
GPU['GPU CUDA Cores'] = 512

cpu['CPU Base clock(GHz)'] = cpu['CPU Base clock(GHz)'].str.replace(r'\s*GHz', '', regex=True).astype(float)
cpu['CPU Max Frequency(GHz)'] = 2.5

input = {
    'CPU': cpu['CPU Model'].iloc[0],
    'GPU': GPU['GPU Model'].iloc[0],
    '# of GPU': 1,
    'GPU Memory Total - VRAM (MB)': GPU['GPU Memory Total - VRAM (MB)'].iloc[0],
    'GPU Graphics clock': GPU['GPU Graphics clock'].iloc[0],
    'GPU Memory clock': GPU['GPU Memory clock'].iloc[0],
    'GPU SM Cores': GPU['GPU SM Cores'].iloc[0],
    'GPU CUDA Cores': GPU['GPU CUDA Cores'].iloc[0],
    'CPU Total cores (Including Logical cores)': cpu['CPU Total cores (Including Logical cores)'].iloc[0],
    'CPU Threads per Core': cpu['CPU Threads per Core'].iloc[0],
    'CPU Base clock(GHz)': cpu['CPU Base clock(GHz)'].iloc[0],
    'CPU Max Frequency(GHz)': cpu['CPU Max Frequency(GHz)'].iloc[0],
    'L1 Cache': cpu['L1 Cache'].iloc[0],
    'Model': 'MICROSOFTPHI-2',
    'Framework': 'PyTorch',
    'Total Parameters (Millions)': 2.58878231,
    'Model Size (MB)': 196340.82,
    'Architecture type': 'PhiForCausalLM',
    'Model Type': 'phi',
    'Number of hidden Layers': 32,
    'Precision': 'FP16',
    'Vocabulary Size': 51200,
    'Number of Attention Layers': 32,
    'Activation Function': 'gelu_new'
}

input = pd.DataFrame(input, index=[0])

#Memory Fit

#Storage fit
disk_usage = shutil.disk_usage("/")
free_disk_space_bytes = (disk_usage.free / 1048576)

bytes_per_param_dict = {
    # Floating Point Types
    "FP64": 8,        # 64-bit float
    "FLOAT64": 8,
    "FP32": 4,        # 32-bit float
    "FLOAT32": 4,
    "FP16": 2,        # 16-bit float
    "FLOAT16": 2,
    "BF16": 2,        # 16-bit bfloat
    "BFLOAT16": 2,

    # Integer Types
    "INT64": 8,       # 64-bit integer
    "INT32": 4,       # 32-bit integer
    "INT16": 2,       # 16-bit integer
    "INT8": 1,        # 8-bit integer
    "UINT8": 1,       # 8-bit unsigned integer

    # Quantized Integer Types
    "QINT32": 4,      # 32-bit quantized integer
    "QINT8": 1,       # 8-bit quantized integer
    "QUINT8": 1,      # 8-bit quantized unsigned integer

    # Complex Types
    "COMPLEX128": 16, # Two 64-bit floats (real + imaginary)
    "COMPLEX64": 8,   # Two 32-bit floats (real + imaginary)

    # Boolean Type
    "BOOL": 1         # Boolean (typically stored as 1 byte)
}

#ram fit
bytes_per_param = bytes_per_param_dict[input['Precision'].iloc[0]]
estimated_model_ram_bytes = (input['Total Parameters (Millions)'].iloc[0] * bytes_per_param)
available_ram_bytes = psutil.virtual_memory().available / 1048576

if input['Model Size (MB)'].iloc[0] < (free_disk_space_bytes):
  if estimated_model_ram_bytes.iloc[0] < available_ram_bytes:
    print('The Model can run in this Hardware specifications')
    run_model = True
  else:
    print('The model cannot run in this hardware specification due to lack of ram available')
else:
  print('The model cannot run in this hardware specification due to lack of storage available')

if run_model:
  input_transformed = pipeline.transform(input)
  Model = pickle.load(open('multi_output_forest.pkl', 'rb'))
  y_pred = Model.predict(input_transformed)
  Output = pd.DataFrame(y_pred, columns=['Throughput (Tokens/sec)', 'Latency (ms)'])

"""# **Recommending hardwares and predicting outputs**"""

available_HW = pd.read_csv('/content/Available_HardwareHPE.csv')

user_input = {
    'Model': 'MICROSOFTPHI-2',
    'Framework': 'PyTorch',
    'Total Parameters (Millions)': 2779.68,
    'Model Size (MB)': 10603.82,
    'Architecture type': 'PhiForCausalLM',
    'Model Type': 'phi',
    'Precision': 'FP16',
    'Vocabulary Size': 51200,
    'Activation Function': 'gelu_new',
    'FLOPs': 26.48
}

user_input = pd.DataFrame(user_input, index=[0])

user_input = pd.concat([user_input]*len(available_HW), ignore_index=True)
user_input = pd.concat([user_input, available_HW], axis=1)

with open('pipeline_inference_preprocessor_simulation.pkl', 'rb') as f:
    pipeline = pickle.load(f)
user_input_transformed = pipeline.transform(user_input)

Model = pickle.load(open('/content/Inference_simulation_model.pkl', 'rb'))
Output = Model.predict(user_input_transformed)

final_table = pd.concat([available_HW, pd.DataFrame(Output, columns=['Throughput (Tokens/sec)', 'Latency (ms)'])], axis=1)

#Computing RAM and Storage:
def recommend_hw(size):
  if size <= 0:
    return "1GB"

  # --- This part is the same as before ---
  log_val = math.log2(size)
  exponent = math.ceil(log_val)
  recommended_mb = 2**exponent

  # --- New part: Convert MB to GB and format the string ---
  recommended_gb = int(recommended_mb / 1024)

  return f"{recommended_gb}"

bytes_per_param_dict = {
    # Floating Point Types
    "FP64": 8,        # 64-bit float
    "FLOAT64": 8,
    "FP32": 4,        # 32-bit float
    "FLOAT32": 4,
    "FP16": 2,        # 16-bit float
    "FLOAT16": 2,
    "BF16": 2,        # 16-bit bfloat
    "BFLOAT16": 2,

    # Integer Types
    "INT64": 8,       # 64-bit integer
    "INT32": 4,       # 32-bit integer
    "INT16": 2,       # 16-bit integer
    "INT8": 1,        # 8-bit integer
    "UINT8": 1,       # 8-bit unsigned integer

    # Quantized Integer Types
    "QINT32": 4,      # 32-bit quantized integer
    "QINT8": 1,       # 8-bit quantized integer
    "QUINT8": 1,      # 8-bit quantized unsigned integer

    # Complex Types
    "COMPLEX128": 16, # Two 64-bit floats (real + imaginary)
    "COMPLEX64": 8,   # Two 32-bit floats (real + imaginary)

    # Boolean Type
    "BOOL": 1         # Boolean (typically stored as 1 byte)
}

bytes_per_param = bytes_per_param_dict[user_input['Precision'].iloc[0]]

user_input['Estimated_RAM'] = (user_input['Total Parameters (Millions)'] * bytes_per_param)

for index, rows in user_input.iterrows():
  user_input['Recommended Storage'] = (recommend_hw(rows['Model Size (MB)']))
  user_input['Recommended RAM'] = recommend_hw(rows['Estimated_RAM'])

user_input['Total power consumption'] = user_input['GPUPower Consumption'] + user_input['CPU Power Consumption']

#Computing cost/1000 inference:
def cost_compute(power_watts, latency_ms, cost_per_kwh=0.12, num_inferences=1000):
  total_time_seconds = (latency_ms / 1000) * num_inferences

  total_time_hours = total_time_seconds / 3600

  power_kw = power_watts / 1000

  total_cost = power_kw * total_time_hours * cost_per_kwh

  return total_cost

final_table.drop_duplicates(inplace=True)
#combine Recommended storage and RAM with final
final_table = pd.merge(final_table, user_input[['Recommended Storage', 'Recommended RAM', 'Total power consumption']], left_index=True, right_index=True)

for index, rows in final_table.iterrows():
  final_table.at[index, 'Total Cost'] = cost_compute(rows['Total power consumption'], rows['Latency (ms)'])

final_table = final_table.sort_values(by=['Total Cost'], ascending=[True])

final_table.reset_index(drop=True, inplace=True)
final_table

"""# **Recommending Top 3 - Pre deployment**"""

final_table.reset_index(drop=True, inplace=True)
top_3_hw = pd.DataFrame()
top_3_hw = final_table.head(3)
top_3_hw

"""# **Post Deployment - Recommendation**"""

user_input_post = {
    'Model Name': 'Microsoft Phi-2',
    'Framework': 'PyTorch',
    'Total Parameters (Millions)': 2779.68,
    'Model Size (MB)': 10603.82,
    'Architecture type': 'PhiForCausalLM',
    'Model Type': 'phi',
    'Precision': 'FP16',
    'Vocabulary Size': 51200,
    'Activation Function': 'gelu_new',
    'gpu_memory_usage' : 90,
    'cpu_memory_usage': 90,
    'cpu_utilization': 90,
    'gpu_utilization': 90,
    'disk_iops': 90,
    'network_bandwidth': 90,
    'current_hardware_id':'NVIDIA L4'
}
user_input_post_pd = pd.DataFrame(user_input_post, index=[0])

def round_off(size):
  if size <= 0:
    return 1
  log_val = math.log2(size)
  exponent = math.ceil(log_val)
  recommended_value = 2**exponent
  return recommended_value

def estimate_resources( total_parameters_b: float, model_disk_size_gb: float, bytes_per_param):

    v_ram_estimate = total_parameters_b * bytes_per_param

    ram_estimate = model_disk_size_gb * 1.5

    return v_ram_estimate, ram_estimate

model_input = user_input_post_pd[['Model Name', 'Framework', 'Total Parameters (Millions)', 'Model Size (MB)', 'Precision']]
bytes_per_param_dict = {
    # Floating Point Types
    "FP64": 8,        # 64-bit float
    "FLOAT64": 8,
    "FP32": 4,        # 32-bit float
    "FLOAT32": 4,
    "FP16": 2,        # 16-bit float
    "FLOAT16": 2,
    "BF16": 2,        # 16-bit bfloat
    "BFLOAT16": 2,

    # Integer Types
    "INT64": 8,       # 64-bit integer
    "INT32": 4,       # 32-bit integer
    "INT16": 2,       # 16-bit integer
    "INT8": 1,        # 8-bit integer
    "UINT8": 1,       # 8-bit unsigned integer

    # Quantized Integer Types
    "QINT32": 4,      # 32-bit quantized integer
    "QINT8": 1,       # 8-bit quantized integer
    "QUINT8": 1,      # 8-bit quantized unsigned integer

    # Complex Types
    "COMPLEX128": 16, # Two 64-bit floats (real + imaginary)
    "COMPLEX64": 8,   # Two 32-bit floats (real + imaginary)

    # Boolean Type
    "BOOL": 1         # Boolean (typically stored as 1 byte)
}

vram, ram = estimate_resources(model_input['Total Parameters (Millions)'].iloc[0]/1000, model_input['Model Size (MB)'].iloc[0]/1024, bytes_per_param_dict[model_input['Precision'].iloc[0]])

if user_input_post['gpu_memory_usage'] >= 80 and user_input_post['cpu_memory_usage'] < 80:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5), "GB")
  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5))


elif user_input_post['gpu_memory_usage'] < 80 and user_input_post['cpu_memory_usage'] >=80:
  try:
    print("To reduce BottleNeck Use:")
    print("RAM:", round_off(ram*1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("RAM:", round_off(ram*1.5), "GB")


elif user_input_post['gpu_memory_usage'] >= 80 and user_input_post['cpu_memory_usage'] >=80:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5), "GB")
    print("RAM:", round_off(ram*1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5), "GB")
    print("RAM:", round_off(ram*1.5), "GB")


    #Downsacle

elif user_input_post['gpu_memory_usage'] < 40 and user_input_post['cpu_memory_usage'] < 40:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")
    print("RAM:", round_off(ram/1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")
    print("RAM:", round_off(ram/1.5), "GB")


elif user_input_post['gpu_memory_usage'] < 40 and user_input_post['cpu_memory_usage'] > 40:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")


elif user_input_post['gpu_memory_usage'] > 40 and user_input_post['cpu_memory_usage'] < 40:
  try:
    print("To reduce BottleNeck Use:")
    print("RAM:", round_off(ram/1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("RAM:", round_off(ram/1.5), "GB")


elif user_input_post['gpu_memory_usage'] >= 80 and user_input_post['cpu_memory_usage'] < 40:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5), "GB")
    print("RAM:", round_off(ram/1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram*1.5), "GB")
    print("RAM:", round_off(ram/1.5), "GB")


elif user_input_post['gpu_memory_usage'] < 40 and user_input_post['cpu_memory_usage'] >=80:
  try:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")
    print("RAM:", round_off(ram*1.5), "GB")

  except:
    print("To reduce BottleNeck Use:")
    print("VRAM:",round_off(vram/1.5), "GB")
    print("RAM:", round_off(ram*1.5), "GB")

hardware_catalog_data = {
    'hardware_id': ['Tesla T4', 'NVIDIA L4', 'NVIDIA A100'],
    'tier_rank': [1, 2, 3]
}
hardware_catalog = pd.DataFrame(hardware_catalog_data).set_index('hardware_id')


UPGRADE_THRESHOLD = 80
DOWNGRADE_THRESHOLD = 30
MAX_TIER = hardware_catalog['tier_rank'].max()
MIN_TIER = hardware_catalog['tier_rank'].min()


def get_hardware_recommendation(gpu_util, mem_util, current_hw_id, catalog):
    if current_hw_id not in catalog.index:
        return f"Error: Hardware ID '{current_hw_id}' not found in catalog."

    current_tier = catalog.loc[current_hw_id, 'tier_rank']

    if gpu_util > UPGRADE_THRESHOLD or mem_util > UPGRADE_THRESHOLD:
        if current_tier == MAX_TIER:
            return 'ALERT: Using the Best Hardware. Try Quantizing'
        else:

            next_tier_id = catalog[catalog['tier_rank'] == current_tier + 1].index[0]
            return f"Upgrade to {next_tier_id}"

    elif gpu_util < DOWNGRADE_THRESHOLD and mem_util < DOWNGRADE_THRESHOLD:
        if current_tier == MIN_TIER:
            return 'ALERT: Using the lowest computing hardware. Continue using the same'
        else:

            prev_tier_id = catalog[catalog['tier_rank'] == current_tier - 1].index[0]
            return f"Downgrade to {prev_tier_id}"
    else:
        return 'MAINTAIN'

get_hardware_recommendation(user_input_post['gpu_utilization'], user_input_post['gpu_memory_usage'], user_input_post['current_hardware_id'], hardware_catalog)

"""# **AI Model for Upgrade/Downgrade**"""

df = pd.read_csv('/content/PostDeployment_Dataset.csv')

X = df.drop('recommendation', axis=1)
y_raw = df['recommendation']

with open('label_encoder_postdeployment.pkl', 'rb') as f:
    le_postdeployment = pickle.load(f)

y = le_postdeployment.transform(y_raw)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

'''preprocessor = ColumnTransformer(
    transformers=[
        ('num', MinMaxScaler(), ['gpu_utilization', 'gpu_memory_usage']),
        ('cat', OneHotEncoder(handle_unknown='ignore'), ['current_hardware_id'])
    ])


pipleline_preprocessor_postdeployment = Pipeline(steps=[
    ('preprocessor', preprocessor)
])

pipleline_preprocessor_postdeployment.fit(X_train)

with open('preprocessor_postDeployment.pkl', 'wb') as f:
    pickle.dump(pipleline_preprocessor_postdeployment, f)
'''

with open('preprocessor_postDeployment.pkl', 'rb') as f:
    pipleline_preprocessor_postdeployment = pickle.load(f)


X_train_transformed = pipleline_preprocessor_postdeployment.transform(X_train)
X_test_transformed = pipleline_preprocessor_postdeployment.transform(X_test)

'''forestClassifier_post = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
forestClassifier_post.fit(X_train_transformed, y_train)

with open('PostDeployement_model.pkl', 'wb') as f:
    pickle.dump(forestClassifier_post, f)'''

with open('PostDeployement_model.pkl', 'rb') as f:
    forestClassifier_post = pickle.load(f)

y_pred = forestClassifier_post.predict(X_test_transformed)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

user_input_post = {
    'gpu_memory_usage' : 90,
    'gpu_utilization': 90,
    'current_hardware_id': 'NVIDIA L4'
}

user_input_post_pd = pd.DataFrame(user_input_post, index=[0])

output = forestClassifier_post.predict(pipleline_preprocessor_postdeployment.transform(user_input_post_pd))

with open ('label_encoder_postdeployment.pkl', 'rb') as f:
  le_postdeployment = pickle.load(f)
Output = pd.DataFrame(le_postdeployment.inverse_transform(output), columns=['Recommended Method'])

method_text = Output['Recommended Method'].iloc[0]
gpu_regex = r"(Tesla T4|NVIDIA L4|NVIDIA A100)"
found_gpus = (re.findall(gpu_regex, method_text, flags=re.IGNORECASE)[0]).upper()

Ouput_row_post = final_table[final_table['GPU'] == found_gpus]
Ouput_row_post

"""# **Model Optimizer**"""

optimizer_input = {
    'Model Name': 'RESNET18',
    'Framework': 'PyTorch',
    'Total Parameters (Millions)': 11.69,
    'Model Size (MB)': 44.59,
    'Architecture type': 'resnet18',
    'Model Type': 'resnet18',
    'Number of hidden Layers': 18,
    'Precision': 'FP32',
    'Vocabulary Size': 1000,
    'Number of Attention Layers': 0,
    'Activation Function':'silu'
}


optimizer_input = pd.DataFrame(optimizer_input, index=[0])

optimizer_dataset = pd.read_csv('/content/optimized_datasetV2.csv')

x = optimizer_dataset.drop(['Recommended Method', 'Recommened Precision'], axis=1)
y = optimizer_dataset[['Recommended Method', 'Recommened Precision']]

'''le_mo = LabelEncoder()
y['Recommended Method'] = le_mo.fit_transform(y['Recommended Method'])

le_pr = LabelEncoder() # Create a new LabelEncoder instance for precision
y['Recommened Precision'] = le_pr.fit_transform(y['Recommened Precision']) # Fit the new instance to precision column

with open('label_encoder_modeloptimizer_method.pkl', 'wb') as f:
    pickle.dump(le_mo, f)

with open('label_encoder_modeloptimizer_precision.pkl', 'wb') as f:
    pickle.dump(le_pr, f)'''

with open('label_encoder_modeloptimizer_method.pkl', 'rb') as f:
    le_mo = pickle.load(f)

with open('label_encoder_modeloptimizer_precision.pkl', 'rb') as f:
    le_pr = pickle.load(f)

y['Recommended Method'] = le_mo.transform(y['Recommended Method'])
y['Recommened Precision'] = le_pr.transform(y['Recommened Precision'])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

'''categorical_features = [
    'Model Name', 'Framework', 'Architecture type',
    'Model Type', 'Precision', 'Activation Function'
]
skewed_numerical_features = [
    'Total Parameters (Millions)', 'Model Size (MB)', 'Number of hidden Layers',
    'Vocabulary Size',
    'Number of Attention Layers'
]


log_transformer = FunctionTransformer(np.log1p)

# Apply MinMaxScaler directly to the 'other_numerical_features'
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ('log', log_transformer, skewed_numerical_features)
    ],
    remainder= 'passthrough'
)


pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])

pipeline.fit(x_train)

with open('pipeline_inference_preprocessor_modeloptimizer.pkl', 'wb') as f:
    pickle.dump(pipeline, f)'''

with open('pipeline_inference_preprocessor_modeloptimizer.pkl', 'rb') as f:
    pipeline_model = pickle.load(f)

x_train_transformed = pipeline_model.transform(x_train)
x_test_transformed = pipeline_model.transform(x_test)

'''forest_classifier_optimizer = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
forest_classifier_optimizer.fit(x_train_transformed.astype(np.float64), y_train)

with open('Optimizer_model.pkl', 'wb') as f:
    pickle.dump(forest_classifier_optimizer, f)'''

optimizer_input_transformed = pipeline_model.transform(optimizer_input)

with open('Optimizer_model.pkl', 'rb') as f:
    forest_classifier_optimizer = pickle.load(f)
output = forest_classifier_optimizer.predict(optimizer_input_transformed)

with open('label_encoder_modeloptimizer_method.pkl', 'rb') as f:
  le_mo = pickle.load(f)

with open('label_encoder_modeloptimizer_precision.pkl', 'rb') as f:
  le_pr = pickle.load(f)

decoded_method = le_mo.inverse_transform(output[:, 0])
decoded_precision = le_pr.inverse_transform(output[:, 1])

Output = pd.DataFrame({
    'Recommended Method': decoded_method,
    'Recommened Precision': decoded_precision
})

Output

